# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTqjB5Q2Kgj6sdC8h0G6cb8PqZ5V2cFq
"""

from google.colab import files
uploaded = files.upload()

# Basic imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# sklearn imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

import pickle
import os

# Read CSV
df = pd.read_csv('measures_v2.csv')
df.head()

print("Shape:", df.shape)
print("\nInfo:")
print(df.info())
print("\nDescribe:")
display(df.describe().T)

#Univariate analysis

# Example: if there is a 'profile_id' or 'session_id' column as category for bar chart:
if 'profile_id' in df.columns:
    plt.figure(figsize=(10,4))
    sns.countplot(y='profile_id', data=df, order=df['profile_id'].value_counts().index[:20])
    plt.title('Top profile_id counts')
    plt.show()
else:
    print("No 'profile_id' column found for countplot example.")

# Boxplots for numeric columns
num_cols = df.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(14,6))
sns.boxplot(data=df[num_cols], orient='h')
plt.title('Boxplots of numeric features (all in one horizontal plot)')
plt.show()

# Distribution plots for a few features (first 4 numeric)
for col in num_cols[:4]:
    plt.figure(figsize=(6,3))
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

#Multivariate analysis

# Scatterplot examples: choose two numeric features
if len(num_cols) >= 2:
    plt.figure(figsize=(6,4))
    sns.scatterplot(x=num_cols[0], y=num_cols[1], data=df, alpha=0.5)
    plt.title(f"{num_cols[0]} vs {num_cols[1]}")
    plt.show()

# Correlation heatmap
plt.figure(figsize=(12,10))
corr = df[num_cols].corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Feature Correlation Heatmap')
plt.show()

#Feature selection / dropping unwanted columns

# Print column names so you can identify which to drop
print(df.columns.tolist())

# Example drop list ‚Äî change these names to match your dataset
drop_cols = []
# Typical columns to drop from modeling inputs (if present):
candidates = ['stator_yoke_temp', 'stator_tooth_temp', 'stator_winding_temp', 'torque', 'profile_id']
for c in candidates:
    if c in df.columns:
        drop_cols.append(c)

print("Dropping columns:", drop_cols)
df_model = df.drop(columns=drop_cols, errors='ignore').copy()

# Check for nulls and basic cleaning

# Null summary
print(df_model.isnull().sum())

# If nulls exist, simple strategies:
# Option A: drop rows with nulls
# df_model = df_model.dropna()

# Option B: fill numeric nulls with median
num_cols_model = df_model.select_dtypes(include=np.number).columns
for c in num_cols_model:
    if df_model[c].isnull().sum() > 0:
        df_model[c].fillna(df_model[c].median(), inplace=True)

#Handling outliers with capping (IQR method)

def cap_outliers_iqr(series):
    q1 = series.quantile(0.25)
    q3 = series.quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    return series.clip(lower, upper)

# Apply to numeric features (except target)
target_col = 'pm'  # change if your rotor temperature column has a different name
if target_col not in df_model.columns:
    # try common names or ask user to confirm
    raise ValueError(f"Target column '{target_col}' not found in dataframe. Rename accordingly.")

features = [c for c in df_model.select_dtypes(include=np.number).columns if c != target_col]
for c in features:
    df_model[c] = cap_outliers_iqr(df_model[c])

# Feature/target split and scaling (MinMaxScaler)

X = df_model[features].values
y = df_model[target_col].values  # numeric target

# MinMax scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Save scaler for later (pickle)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("Scaler saved to scaler.pkl")

#Train-test split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

#Model Building

def evaluate_model(model, X_test, y_test):
    preds = model.predict(X_test)
    rmse = mean_squared_error(y_test, preds, squared=False)
    r2 = r2_score(y_test, preds)
    return rmse, r2

results = {}

from sklearn.metrics import mean_squared_error
import numpy as np

# Predict first
y_pred = lr.predict(X_test)

# Calculate RMSE (works with all scikit-learn versions)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import pickle

# 1Ô∏è‚É£ Load dataset
df = pd.read_csv("measures_v2.csv")

# 2Ô∏è‚É£ Reduce dataset size for speed (10000 rows for most models)
df = df.sample(n=10000, random_state=42)

# 3Ô∏è‚É£ Set target column
target_column = "profile_id"  # change if needed
X = df.drop(columns=[target_column])
y = df[target_column]

# 4Ô∏è‚É£ Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5Ô∏è‚É£ Define models
models = {
    "LinearRegression": LinearRegression(),
    "DecisionTree": DecisionTreeRegressor(max_depth=5, random_state=42),
    "RandomForest": RandomForestRegressor(
        n_estimators=20, max_depth=5, n_jobs=-1, random_state=42
    ),
    "SVR": SVR(kernel="rbf", C=1.0, epsilon=0.1)
}

results = {}

# 6Ô∏è‚É£ Train & evaluate
for name, model in models.items():
    print(f"Training {name} ...")

    # For SVR, train on only 5000 rows to make it fast
    if name == "SVR":
        X_train_small = X_train.sample(n=5000, random_state=42)
        y_train_small = y_train.loc[X_train_small.index]
        model.fit(X_train_small, y_train_small)
    else:
        model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred) ** 0.5
    r2 = r2_score(y_test, y_pred)
    results[name] = {"RMSE": rmse, "R2": r2}

# 7Ô∏è‚É£ Show results
results_df = pd.DataFrame(results).T
print("\nModel Performance:")
print(results_df)

# 8Ô∏è‚É£ Save best model
best_model_name = results_df["R2"].idxmax()
best_model = models[best_model_name]

with open("model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print(f"\n‚úÖ Best model '{best_model_name}' saved as model.pkl")

import pandas as pd
import pickle

# 1Ô∏è‚É£ Load the saved model
with open("model.pkl", "rb") as f:
    model = pickle.load(f)

print("‚úÖ Model loaded successfully!")

# 2Ô∏è‚É£ Load dataset (same one or a new one)
df = pd.read_csv("measures_v2.csv")

# 3Ô∏è‚É£ Use same target column as in training
target_column = "profile_id"

# 4Ô∏è‚É£ Prepare features (drop target)
X = df.drop(columns=[target_column])

# 5Ô∏è‚É£ Simple test prediction (first 5 rows)
predictions = model.predict(X.head(5))

print("\nüîπ Sample Predictions:")
for i, pred in enumerate(predictions, start=1):
    print(f"Row {i}: {pred}")

import pickle

with open("model.pkl", "wb") as f:
    pickle.dump({"model": best_model, "scaler": scaler}, f)

with open("model.pkl", "rb") as f:
    data = pickle.load(f)
model = data["model"]
scaler = data["scaler"]

# Save both model and scaler separately
import pickle

with open("model.pkl", "wb") as f:
    pickle.dump(best_model, f)

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

print("‚úÖ model.pkl and scaler.pkl saved successfully")

from google.colab import files
uploaded = files.upload()

import pickle

# Save both the trained scaler and the trained model
with open("model.pkl", "wb") as f:
    pickle.dump(best_model, f)

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

print("‚úÖ model.pkl and scaler.pkl saved successfully")

import pickle

with open("model.pkl", "wb") as f:
    pickle.dump({"model": best_model, "scaler": scaler}, f)

print("‚úÖ model.pkl saved successfully")

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# 1Ô∏è‚É£ Load dataset
df = pd.read_csv("measures_v2.csv")
target_column = "profile_id"
X = df.drop(columns=[target_column])
y = df[target_column]

# 2Ô∏è‚É£ Scale features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# 3Ô∏è‚É£ Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Reduce training size for all models
train_limit = 5000
X_train_small = X_train[:train_limit]
y_train_small = y_train[:train_limit]

# 4Ô∏è‚É£ Define models
models = {
    "LinearRegression": LinearRegression(),
    "DecisionTree": DecisionTreeRegressor(max_depth=5, random_state=42),
    "RandomForest": RandomForestRegressor(n_estimators=20, max_depth=5, n_jobs=-1, random_state=42),
    "SVR": SVR(kernel="rbf", C=1.0, epsilon=0.1)
}

# 5Ô∏è‚É£ Train & evaluate
results = {}
for name, model in models.items():
    print(f"Training {name} on {train_limit} rows ...")
    model.fit(X_train_small, y_train_small)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    results[name] = rmse
    print(f"{name} RMSE: {rmse:.4f}")

# 6Ô∏è‚É£ Pick best model
best_model_name = min(results, key=results.get)
best_model = models[best_model_name]

print(f"\n‚úÖ Best model: {best_model_name}")
print(f"‚úÖ Best model RMSE: {results[best_model_name]:.4f}")

# 7Ô∏è‚É£ Save best model & scaler
joblib.dump(best_model, "model.save")
joblib.dump(scaler, "transform.save")
print("‚úÖ Model saved as model.save")
print("‚úÖ Scaler saved as transform.save")